{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from utils.loader import lvlm_loader\n",
    "from utils.utils import generate, return_attention, compute_tile_weights, return_weights, valuable_weights\n",
    "from utils.metrics import compute_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from transformers import GenerationConfig\n",
    "from transformers import StoppingCriteria, StoppingCriteriaList\n",
    "\n",
    "class StopOnToken(StoppingCriteria):\n",
    "    def __init__(self, stop_token_id):\n",
    "        self.stop_token_id = stop_token_id\n",
    "\n",
    "    def __call__(self, input_ids, scores, **kwargs):\n",
    "        if input_ids[0][-1] == self.stop_token_id:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config():\n",
    "    model_id = '' \n",
    "args=Config()\n",
    "args.model_id = 'Qwen/Qwen2-VL-7B-Instruct'\n",
    "args.bos_as_weight = False\n",
    "args.norm = False\n",
    "args.do_ablation = False\n",
    "args.fine_grained = True\n",
    "args.do_masking = False\n",
    "args.do_ablation = False\n",
    "ppp = [\"img_dataset_an\"]\n",
    "\n",
    "def load_data(metadata, number, fg = False):\n",
    "    path = 'img_dataset_an/'\n",
    "    img_paths = sorted(glob(path + f'{number}/*'))\n",
    "    evidence_page = int(list(metadata[number][f'{str(number)}']['bboxes_per_chunk'].keys())[0])\n",
    "    bbox = list(metadata[number][f'{str(number)}']['bboxes_per_chunk'].values())[0]\n",
    "    answer = metadata[number][f'{str(number)}'][\"answer\"]\n",
    "    try:\n",
    "        question = metadata[number][\"question\"]\n",
    "    except: question = ''\n",
    "    return img_paths, evidence_page, bbox, answer, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model, processor = lvlm_loader(args.model_id, device = device, args = args)\n",
    "model.eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(args.model_id, trust_remote_code=True)\n",
    "\n",
    "args.processor = processor\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.gradient_checkpointing_disable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'intern' in args.model_id.lower():\n",
    "    n_heads = model.config.llm_config.num_attention_heads\n",
    "    n_layer = model.config.llm_config.num_hidden_layers\n",
    "    q_s, q_i, q_e = 92544, 92546, 92545\n",
    "else:\n",
    "    n_heads = model.config.num_attention_heads # 10\n",
    "    n_layer = model.config.num_hidden_layers # 12\n",
    "    q_s, q_i, q_e = 151652, 151655, 151653\n",
    "import string\n",
    "chars = list(string.digits + string.ascii_lowercase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, mean_features = {}, {}\n",
    "for ch in chars:\n",
    "    features[ch] = {}\n",
    "    mean_features[ch] = {}\n",
    "    for layer in range(n_layer):\n",
    "        for head in range(n_heads):\n",
    "            features[ch][f'l{layer}_h{head}'] = []\n",
    "            mean_features[ch][f'l{layer}_h{head}'] = []\n",
    "        \n",
    "pp = \"img_dataset_an\"\n",
    "_number = 0\n",
    "if args.fine_grained: metadata_path = f\"{pp}/bbox_fg.json\"\n",
    "else: metadata_path = f\"{pp}/bbox.json\"\n",
    "with open(metadata_path, 'r') as f:\n",
    "    metadata = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = []\n",
    "for number in tqdm(range(len(metadata))):\n",
    "    if metadata[number][f'{str(number)}']['bboxes_per_chunk'] == {}:\n",
    "        continue\n",
    "    if _number == 0:\n",
    "        img_paths, evidence_page, bbox, answer, _ = load_data(metadata, number)\n",
    "    else: img_paths, evidence_page, bbox, answer, question = load_data(metadata, number)\n",
    "    label_img_path = img_paths[evidence_page]\n",
    "    label_img=Image.open(label_img_path)\n",
    "    label_array=np.array(label_img)\n",
    "    args.label_array = label_array\n",
    "    \n",
    "    if args.fine_grained:\n",
    "        input_content = [{\"type\": \"image\", \"image\": f'''{label_img_path}'''}]\n",
    "        messages = [{\"role\": \"user\", \"content\": input_content}]\n",
    "        weights_list, new_weight_list = [], []\n",
    "        \n",
    "        for box_len in range(len(bbox)):\n",
    "            xmin, ymin, xmax, ymax = bbox[box_len]\n",
    "            weights=return_weights(args, messages, xmin, ymin, xmax, ymax)\n",
    "            weights_list.append(weights)\n",
    "            new_weight = valuable_weights(weights)\n",
    "            new_weight_list.append(new_weight)\n",
    "                    \n",
    "    else:\n",
    "        xmin, ymin, xmax, ymax = bbox\n",
    "\n",
    "        input_content = [{\"type\": \"image\", \"image\": f'''{label_img_path}'''}]\n",
    "        messages = [{\"role\": \"user\", \"content\": input_content}]\n",
    "        weights=return_weights(args, messages, xmin, ymin, xmax, ymax)\n",
    "        new_weight = valuable_weights(weights)\n",
    "        \n",
    "    \n",
    "    if 'qwen' in args.model_id.lower():\n",
    "        input_content = [{\"type\": \"image\", \"image\": f'''{path}'''} for path in img_paths]\n",
    "        \n",
    "        if _number == 0:\n",
    "            input_content.append({\"type\": \"text\", \"text\": '''What is the pass key?'''})\n",
    "        else:\n",
    "            input_content.append({\"type\": \"text\", \"text\": f'''{question}'''})\n",
    "            \n",
    "        outputs, generated_ids_trimmed, output_text, with_sp_output_text, inputs = generate(args, model, tokenizer, processor, input_content=input_content)\n",
    "        start_ids = torch.nonzero(inputs.input_ids.squeeze().to('cpu') == q_s).squeeze() +1\n",
    "        end_ids = torch.nonzero(inputs.input_ids.squeeze().to('cpu') == q_e).squeeze() -1\n",
    "        \n",
    "    elif 'internvl' in args.model_id.lower():\n",
    "        \n",
    "        num_patches = len(img_paths)\n",
    "        num_image_token = model.num_image_token\n",
    "        IMG_START_TOKEN = \"<img>\"\n",
    "        IMG_END_TOKEN   = \"</img>\"\n",
    "        IMG_CONTEXT_TOKEN = \"<IMG_CONTEXT>\"\n",
    "\n",
    "        image_tokens = IMG_START_TOKEN + (IMG_CONTEXT_TOKEN * (num_image_token * num_patches)) + IMG_END_TOKEN\n",
    "        if _number == 0:\n",
    "            user_question = \"What is the passkey?\"\n",
    "        else: user_question = f\"{question}\"\n",
    "        \n",
    "        prompt = f\"{image_tokens}\\n{user_question}\" \n",
    "        model_inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = model_inputs[\"input_ids\"].to(\"cuda\")\n",
    "        attention_mask = model_inputs[\"attention_mask\"].to(\"cuda\")\n",
    "        \n",
    "        model.img_context_token_id = tokenizer.convert_tokens_to_ids(IMG_CONTEXT_TOKEN)   \n",
    "        generation_config = GenerationConfig(\n",
    "            max_new_tokens=32,\n",
    "            do_sample=False,\n",
    "        )\n",
    "        from utils.utils import load_image\n",
    "        pixel_values = torch.cat([load_image(path, max_num = 1).to(torch.float16).cuda() for path in img_paths], dim = 0)\n",
    "        stop_token_id = 281 \n",
    "        stopping_criteria = StoppingCriteriaList([StopOnToken(stop_token_id)])\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                pixel_values=pixel_values,\n",
    "                input_ids=input_ids,   \n",
    "                attention_mask=attention_mask,  \n",
    "                return_dict_in_generate=True,\n",
    "                output_attentions=True,\n",
    "                generation_config=generation_config,\n",
    "                stopping_criteria=stopping_criteria\n",
    "            )\n",
    "        generated_ids_trimmed=[outputs.sequences.squeeze(dim = 0)]\n",
    "        inputs = input_ids\n",
    "            \n",
    "        n_of_imgs= int(torch.sum(torch.where(input_ids == 92546, 1, 0)).item() / 256)\n",
    "        start_num = torch.nonzero(input_ids.squeeze().to('cpu') == q_s).item() +1\n",
    "        start_ids = start_num + 256 * torch.arange(0, n_of_imgs)\n",
    "        \n",
    "\n",
    "    attentions = outputs['attentions']\n",
    "    \n",
    "    pattern_list = tokenizer(str(answer))\n",
    "    \n",
    "    tmp, context_idx, ans_idx = [], [], []\n",
    "   \n",
    "    for n, tok in enumerate(generated_ids_trimmed[0].to('cpu').tolist()):\n",
    "        if 'qwen' in args.model_id.lower():\n",
    "            if tok in pattern_list.input_ids:\n",
    "                ans_idx.append(inputs.input_ids.shape[1] + n)\n",
    "                \n",
    "        elif 'internvl' in args.model_id.lower():\n",
    "            if tok in pattern_list.input_ids[1:]:\n",
    "                ans_idx.append(inputs.shape[1] + n)\n",
    "                \n",
    "    for layer in range(n_layer):\n",
    "        for head in range(n_heads):\n",
    "            \n",
    "            attn = return_attention(args.model_id, attentions, layer = layer, head_n = head, \n",
    "                                    norm = args.norm, bos_as_weight= args.bos_as_weight)\n",
    "            if 'qwen' in args.model_id.lower():\n",
    "                image_retrieval_score = compute_score(pattern_list.input_ids, outputs.sequences.squeeze().to('cpu').tolist())\n",
    "            elif 'internvl' in args.model_id.lower():\n",
    "                image_retrieval_score = compute_score(pattern_list.input_ids[1:], outputs.sequences.squeeze().to('cpu').tolist())\n",
    "            \n",
    "            if args.fine_grained:\n",
    "                for box_len in range(len(bbox)):\n",
    "                    pos_attn_position = set([weight + start_ids[evidence_page].item() for weight in list(new_weight_list[box_len].keys())])\n",
    "                    \n",
    "                    score = 0\n",
    "                    try:\n",
    "                        index = torch.argmax(attn[ans_idx[box_len],:-len(generated_ids_trimmed[0])]).item()            \n",
    "                    except:\n",
    "                        continue\n",
    "                    if index in pos_attn_position:\n",
    "                        features[answer][f\"l{str(layer)}_h{str(head)}\"].append(image_retrieval_score)\n",
    "                        \n",
    "                    else:\n",
    "                        features[answer][f\"l{str(layer)}_h{str(head)}\"].append(0)\n",
    "                    \n",
    "                        \n",
    "            else:\n",
    "                pos_attn_position = set([weight + start_ids[evidence_page].item() for weight in list(new_weight.keys())])\n",
    "                score = 0\n",
    "                for ans in ans_idx:\n",
    "                    index = torch.argmax(attn[ans,:-len(generated_ids_trimmed[0])]).item()            \n",
    "                    \n",
    "                    if index in pos_attn_position:\n",
    "                        features[answer][f\"l{str(layer)}_h{str(head)}\"].append(image_retrieval_score)\n",
    "                    else:\n",
    "                        features[answer][f\"l{str(layer)}_h{str(head)}\"].append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "for ch, feats in features.items():\n",
    "    for head_name, values in feats.items():\n",
    "\n",
    "        if values:\n",
    "            mean_features[ch][head_name] = np.mean(values)\n",
    "        else:\n",
    "            mean_features[ch][head_name] = float(\"-inf\")  \n",
    "\n",
    "\n",
    "for ch, means in mean_features.items():\n",
    "    \n",
    "    sorted_heads = sorted(\n",
    "        means.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "    top5 = sorted_heads[:5]\n",
    "    \n",
    "    print(f\"Character '{ch}' top-5 heads:\")\n",
    "    for head_name, mean_val in top5:\n",
    "        print(f\"  {head_name}: {mean_val:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_char_heatmaps(mean_features, n_layers, n_heads, output_dir=\"char_heatmaps\"):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    for ch, heads_dict in mean_features.items():\n",
    "        mat = np.zeros((n_layers, n_heads))\n",
    "        for layer in range(n_layers):\n",
    "            for head in range(n_heads):\n",
    "                key = f\"l{layer}_h{head}\"\n",
    "                mat[layer, head] = heads_dict.get(key, 0.0)\n",
    "\n",
    "        # 2) figure 생성\n",
    "        plt.figure()\n",
    "        plt.imshow(mat, aspect=\"auto\") \n",
    "        plt.title(f\"Heatmap for '{ch}'\", pad=8)\n",
    "        plt.xlabel(\"Head index\")\n",
    "        plt.ylabel(\"Layer index\")\n",
    "        plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top5_heads(mean_features, output_dir=\"plots_top5\"):\n",
    "\n",
    "    import os\n",
    "    os.makedirs(output_dir, exist_ok=True) \n",
    "\n",
    "    for ch, heads_dict in mean_features.items():\n",
    "        \n",
    "        sorted_heads = sorted(heads_dict.items(), key=lambda x: x[1], reverse=True)[:5]\n",
    "        labels, values = zip(*sorted_heads)\n",
    "\n",
    "        plt.figure()\n",
    "        plt.bar(labels, values)\n",
    "        plt.title(f\"'{ch}' top-5 heads\", pad=8)\n",
    "        plt.xlabel(\"Head\")\n",
    "        plt.ylabel(\"Mean value\")\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{output_dir}/top5_{ch}.png\", dpi=300)\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os, math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def plot_char_top5_heatmaps_grid(\n",
    "    mean_features: dict,\n",
    "    n_layers: int,\n",
    "    n_heads: int,\n",
    "    chars: list = None,\n",
    "    cols: int = 6,\n",
    "    figsize: tuple = (18, 18),\n",
    "    cmap: str = \"viridis\",\n",
    "    save_path: str = None\n",
    "):\n",
    "    if chars is None:\n",
    "        chars = sorted(mean_features.keys())\n",
    "    total = len(chars)\n",
    "    rows = math.ceil(total / cols)\n",
    "\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize, squeeze=False)\n",
    "\n",
    "    all_vals = [\n",
    "        mean_features[ch].get(f\"l{l}_h{h}\", 0.0)\n",
    "        for ch in chars\n",
    "        for l in range(n_layers)\n",
    "        for h in range(n_heads)\n",
    "    ]\n",
    "    vmin, vmax = min(all_vals), max(all_vals)\n",
    "\n",
    "    for idx, ch in enumerate(chars):\n",
    "        r, c = divmod(idx, cols)\n",
    "        ax = axes[r][c]\n",
    "\n",
    "        mat = np.array([\n",
    "            [mean_features[ch].get(f\"l{l}_h{h}\", 0.0) for h in range(n_heads)]\n",
    "            for l in range(n_layers)\n",
    "        ])\n",
    "\n",
    "        flat = mat.flatten()\n",
    "        sorted_items = sorted(enumerate(flat), key=lambda x: x[1], reverse=True)\n",
    "        top5 = sorted_items[:5]\n",
    "        mask = np.zeros_like(mat, dtype=bool)\n",
    "        for idx_flat, _ in top5:\n",
    "            l = idx_flat // n_heads\n",
    "            h = idx_flat % n_heads\n",
    "            mask[l, h] = True\n",
    "        vis_mat = np.where(mask, mat, 0)\n",
    "\n",
    "        # 3) 히트맵\n",
    "        im = ax.imshow(vis_mat, aspect=\"auto\", vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "\n",
    "        ax.set_xticks(np.arange(n_heads))\n",
    "        ax.set_xticklabels([f\"{h}\" for h in range(n_heads)],\n",
    "                           fontsize=4, rotation=90)\n",
    "        ax.set_yticks(np.arange(n_layers))\n",
    "        ax.set_yticklabels([f\"{l}\" for l in range(n_layers)],\n",
    "                           fontsize=4)\n",
    "\n",
    "        ax.set_title(ch, pad=2, fontsize=16)\n",
    "        \n",
    "        lines = [f\"L{idx_flat//n_heads}H{idx_flat%n_heads}: {val:.2f}\"\n",
    "                 for idx_flat, val in top5]\n",
    "        annotation = \"\\n\".join(lines)\n",
    "        ax.text(\n",
    "            0.01, 0.01, annotation,\n",
    "            transform=ax.transAxes,\n",
    "            va=\"bottom\", ha=\"left\",\n",
    "            color=\"white\", fontsize=10,\n",
    "            bbox=dict(facecolor=\"black\", alpha=0.5, pad=2)\n",
    "        )\n",
    "\n",
    "    for idx in range(total, rows * cols):\n",
    "        r, c = divmod(idx, cols)\n",
    "        axes[r][c].axis(\"off\")\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300)\n",
    "    plt.show()\n",
    "chars = list(string.digits + string.ascii_lowercase)\n",
    "sample = next(iter(mean_features.values()))\n",
    "layers = [int(k.split('_')[0][1:]) for k in sample.keys()]\n",
    "heads  = [int(k.split('_')[1][1:]) for k in sample.keys()]\n",
    "n_layers = max(layers) + 1\n",
    "n_heads  = max(heads)  + 1\n",
    "\n",
    "plot_char_top5_heatmaps_grid(\n",
    "    mean_features=mean_features,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    chars=chars,\n",
    "    cols=6,\n",
    "    figsize=(15, 20),\n",
    "    cmap=\"viridis\",\n",
    "    save_path=\"figure/top5_heatmap_grid.pdf\" \n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
